## Spark源码



### 一、Spark RPC

Spark 底层的 RPC 框架在 2.x 版本中进行了大的升级，Spark2.x之前基于Akka，Spark2.x之后基于Netty，Spark1.6 时已经提供了netty的实现，可供用户选择，但不是默认方式，2.x之后默认使用Netty，去除了Akka，总结的原因如下：

- Akka 不同版本之间无法互相通信，这就要求用户必须使用跟 Spark 完全一样的 Akka 版本，导致用户无法升级 Akka。
- Spark 的 Akka 配置是针对Spark自身来调优的，可能跟用户自己代码中的 Akka 配置冲突。
- Spark 官方对此的描述是："Akka的依 赖被移除了，因此用户可以使用任何版本的 Akka 来进行编程了。"

从Akka出现背景来说，它是基于Actor的RPC通信系统，它的核心概念也是Message，它是基于协程的，性能不容置疑；基于Scala的偏函数，易用性也没有话说，但是它毕竟只是RPC通信，无法适用大的package/stream的数据传输，这也是Spark早期引入Netty的原因。

参考资料：

Spark为何使用Netty通信框架替代Akka：https://blog.csdn.net/psiitoy/article/details/77530886



#### 1. Akka RPC 实现

Akka 在 Scala 中的地位就像 Netty 在 Java 中的地位，Akka 是一个基于事件驱动的异步消息模型

1. Akka 原理

   ```
   1、它是对并发模型进行了更高的抽象；
   2、它是异步、非阻塞、高性能的事件驱动编程模型；
   3、它是轻量级事件处理(1GB 内存可容纳百万级别个 Actor);
   ```

2. Akka 的组件

   ```
   ActorSystem: 管理和创建 Actor
   Actor：负责通信，内部有 MailBox 和 ActorRef 帮助完成通信，也可以创建 Actor
   ActorRef：通信代理
   MailBox：存放消息的地方，使得处理任务可以异步执行
   ```

3. 关于对 Akka 的 ActorSystem 和 Actor 的理解：

   ```
   1、ActorSystem 是管理 Actor 生命周期的组件，Actor 是负责进行通信的组件
   2、每个 Actor 都有一个 MailBox，别的 Actor 发送给它的消息都首先储存在 MailBox 中，通过这种方式可以实现异步通信。
   3、每个 Actor 是单线程的处理方式，不断的从 MailBox 拉取消息执行处理，所以对于 Actor的消息处理，不适合调用会阻塞的处理方法
   4、Actor 可以改变他自身的状态，可以接收消息，也可以发送消息，还可以生成新的 Actor
   5、每一个 ActorSystem 和 Actor 都在启动的时候会给定一个 name，如果要从 ActorSystem 中，获取一个 Actor，则通过以下的方式来进行 Actor 的获取：akka.tcp://actorsystem_name@bigdata02:9527/user/actor_name
   6、如果一个 Actor 要和另外一个 Actor 进行通信，则必须先获取对方 Actor 的 ActorRef 对象，然后通过该对象发送消息即可
   7、通过 tell 发送异步消息，不接收响应，通过 ask 发送异步消息，得到 Future 返回，通过异步回到返回处理结果
   8、其实每一个 ActorSystem 就是存在于 JVM 中，用来管理 Actor 通信组件生命周期
   9、Actor 通信单元：A 给 B 发送消息，同时 B 也可以给 A 返回消息，A 和 B 都是通信单元
   ```

   

4. 流程图

   ![image-20221206003303132](D:\WorkSpace\GitHubWorkSpace\spark-study\spark-nx\image\笔记\image-20221206003303132.png)



心跳机制：Worker 节点给 Master 节点发送心跳，Master 更新 Worker 最近心跳时间 

验活机制：Master 节点遍历自己维护的 Worker List ，找出最近心跳时间已经超过某个时间范围的节点做下线处理



#### 2. Spark RPC 实现

​		在 Spark 的内部的很多地方都涉及网络通信，比如 Spark 各个组件之间的互相通信，用户文件与 Jar 包的上传，节点之间的 Shuffle 过程，Block 数据的复制和备份。在 Spark 0.x 和 Spark 1.x 版本中，使用 Akka 轻松构建强有力的高并发和分布式网络通信，但是在 Spark 2.x 被移除了。

1. Spark 进程之间的通信

   ```
   1. driver和master：比如driver会向master发送RegisterApplication消息
   2. Worker和master：比如worker会向master上报worker上运行Executor信息
   3. executor和driver：executor运行在worker上，spark的tasks被分发到运行在各个Executor中，executor需要通过向driver发送任务运行结果
   4. worker和worker：task运行期间需要从其他地方fetch数据，这些数据是由运行在其他worker上的executor上的task产生，因此需要到worker上fetch数据
   ```

2. Spark RPC 组件

   Spark RPC 客户端的具体实现是 TransportClient，由 TransportClientFactory 创建，TransportClientFactory 在实例化的时候需要 TransportConf，创建好了之后，需要通过 TransportClientBootstrap 引导启动，创建好的 TransportClient 都会被维护在
   TransportClientFactory 的 ClientPool 中。

   Spark RPC 服务端的具体实现是 TransportServer，创建的时候，需要 TransportContext 中的 TransportConf 和 RpcHandler。在 init 初始化的时候，由 TransportServerBootStrap 引导启动。

   在 Spark RPC 过程中，实现具体的编解码动作的是： MessageEncoder 和 MessageDecoder。可以集成多种不同的序列化机制。

   （1）：**TransportClient**

   ```java
   public class TransportClient implements Closeable {
       // 重要的成员变量
       private final Channel channel;
       private final TransportResponseHandler handler;
       @Nullable private String clientId;
       private volatile boolean timedOut;
       ...
   }
   ```

   （2）：**TransportServer**

   ```java
   public class TransportServer implements Closeable {
       // Transport 上下文对象
       private final TransportContext context;
       // Transport 配置对象
       private final TransportConf conf;
       private final RpcHandler appRpcHandler;
       // TransportServer 的引导启动对象
       private final List<TransportServerBootstrap> bootstraps;
   	// Netty Server 的启动引导对象
       private ServerBootstrap bootstrap;
       private ChannelFuture channelFuture;
       private int port = -1;
       private NettyMemoryMetrics metrics;
       ...
   ```

   （3）：**TransportClientFactory**

   ```java
   
   public class TransportClientFactory implements Closeable {
       // 重要的成员变量
       // Transport 上下文对象
       private final TransportContext context;
       // Transport 配置对象
       private final TransportConf conf;
       // TransportClient 的引导启动对象
       private final List<TransportClientBootstrap> clientBootstraps;
       // TransportClient 缓存池
       private final ConcurrentHashMap<SocketAddress, ClientPool> connectionPool;
   	...
   
   ```

   （4）：**ClientPool**

   ```java
   // ClientPool 是 TransportClientFactory 的一个内部类，用数组存放 TransportClient
   private static class ClientPool {
       TransportClient[] clients;
       Object[] locks;
       
       ClientPool(int size) {
       clients = new TransportClient[size];
       locks = new Object[size];
       for (int i = 0; i < size; i++) {
           locks[i] = new Object();
       }
     }
   }
   ```

   （5）：**TransportClientBootstrap**

   ```java
   public interface TransportClientBootstrap {
     /** Performs the bootstrapping operation, throwing an exception on failure. */
     // 有三个实现类，用来执行执行引导操作
     void doBootstrap(TransportClient client, Channel channel) throws RuntimeException;
   }
   ```

   





#### 3. Spark Standalone 集群启动源码

向Spark 集群提交一个 Application的时候，从jar包中指定的Main方法开始执行，遇到一个Action算子就会触发一个Job的提交，提交Job的时候会把 Job RDD根据算子的类型串起来，构建出一个有向无环图DAG，这个DAG就会交给Driver中的DAGScheduler做Stage切分变成多个Stage，然后遍历Stage，把每个Stage交给TaskScheduler，TaskScheduler将Stage转化成Task Set，发送到对应的Executor执行

1. 架构设计

   SparkContext有三个重要的成员变量，DAGScheduler、TaskScheduler、SchedulerBackend



Worker验活机制：当Worker被判定为下线之后，Master在下线处理前会将这个Worker上运行的Driver或者Executor进行容错处理，也就是发送给其他节点去启动一个新的，然后再下线这个Worker



### 二、Spark Application 提交和部署



#### 1. Spark DAG 引擎

spark 比 mapreduce 快的原因有两点：基于内存，DAG引擎

把rdd的链式调用变成一个有向无环图











### 四、Spark Shuffle



#### 1. MapReduce Shuffle

默认一个文件块一个mapTask,最后一个文件大小如果小于Block的10%，则合并到上一个MapTask

Block -> MapTask ->  100M的环形缓冲区 -> 达到80%容量 -> 排序 生成一个文件 分区编号和主键-> 归并这些文件为一个有序的文件和索引文件 -> ReduceTask读取



对文件块进行逻辑切片，每个切片启动一个MapTask，MapTask启动完之后会根据自己负责的逻辑切片去扫描文件，然后执行计算，MapTask执行完计算之后会调用Partition对输出的数据打上分区的标记，写到一个大小100M的环形缓冲区，目的是为了排序，当环形缓冲区达到80%的时候进行排序，然后spill到文件中，那么这个spill文件是局部有序的，最后将多个spill文件进行归并排序，并生成一个索引文件，这样一个MapTask就只有一个输出文件了，默认是3个文件开始进行合并，也可以配置不同的策略。

ReduceTask从上游拉取属于自己的文件做归并，最后形成一个大的文件，这样执行Reduce逻辑的时候只需要顺序扫描一次就可以了，如果不执行排序，则每计算一个key就要重复扫描全量的文件

Partition常见规则：

- 随机

- 轮询

- Hash

- 范围 : 可以实现

- 广播

  ...

必须保证ReduceTask输入数据有序

为了分摊排序压力，所以MapTask输出的时候也要排序，输出的最终文件是分段有序的，然后在生成一个索引文件每段的起始偏移量

缺点：

只要有Shuffle就一定要排序，为了提高ReduceTask的效率

#### 2. Spark Shuffle



SortShuffle Bypass模式：Task写出的时候这么知道哪段写到哪里













### 五、Spark 内存模型

Spark 对堆内内存的管理是一种逻辑上的**"规划式"**的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存。

在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 SparkContext，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。

#### 1. 静态内存管理模型（StaticMemoryManager）

Spark 1.6 之前采用静态内存管理模型，其特点是：存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的。

![image-20221209231727834](D:\WorkSpace\GitHubWorkSpace\spark-study\spark-nx\image\笔记\image-20221209231727834.png)

堆外内存中可用的执行内存和存储内存占用的空间可以被精确计算，所以没有Other区域

1. **堆内内存中可用的存储内存**

   StorageMemory = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction = 43.2%

2. **堆内内存中可用的执行内存**

   ExecutionMemory = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction = 16%

3. **静态内存管理模型优缺点**

   缺点：虽然各个区域的内存可以调节，但是开发人员要非常熟悉Spark的内存分配机制，而且还要根据作业的规模做出合理的配置，如果配置不合理就会造成旱的旱死，涝的涝死。Spark 为了兼容老版本应用程序的目的仍然保留了这种内存模型的实现和使用。

#### 2. 统一内存管理模型（UnifiedMemoryManager）

Spark1.6之后引入统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域。

![image-20221209231657959](D:\WorkSpace\GitHubWorkSpace\spark-study\spark-nx\image\笔记\image-20221209231657959.png)

**动态占用机制**

1. 优先级：**Execution 内存 > Storage 内存**，意味着 执行内存 如果被  存储内存 占用了，可以要求归还，反之不能。
2. 存储内存 和 执行内存 在对方还有空闲而自己使用满的情况下都可以占用对方的内存，但是只有 执行内存 可以要求 存储内存 归还，
3. 如果 存储内存 和 执行内存 都不足，则他们会使用磁盘



#### 3. MemoryManager

UnifiedMemoryManager 内存模型内各个区域的划分，就是由MemoryManager 决定的，所以我们从 MemoryManager 入手

我们平时创建SparkSession的时候，内部会初始化 SparkContext，SparkContext 初始化的时候会创建 SparkEnv，我们的MemoryManager就是在SparkEnv里创建的，接下来我们就打开源码：org.apache.spark.SparkEnv#create

```scala
SparkEnv.create(){
	...
    // TODO useLegacyMemoryManager：是否使用过期的内存管理器，默认是否，所以创建的是UnifiedMemoryManager
    val useLegacyMemoryManager = conf.getBoolean("spark.memory.useLegacyMode", false)
    val memoryManager: MemoryManager =
    if (useLegacyMemoryManager) {
        new StaticMemoryManager(conf, numUsableCores)
    } else {
        UnifiedMemoryManager(conf, numUsableCores)
    }
}
```

我们继续跟进UnifiedMemoryManager(conf, numUsableCores)

```scala
  // TODO 系統保留内存 300M
  private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024

  def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = {
    // TODO 获取 (Storage + Execution) 最大可使用的内存 = ( heap size - 300M ) * 0.6
    val maxMemory = getMaxMemory(conf)
    new UnifiedMemoryManager(
      conf,
      // TODO 统一内存等于最大可使用内存
      maxHeapMemory = maxMemory,
      // TODO 存储内存占统一内存的 50%
      onHeapStorageRegionSize =
        (maxMemory * conf.getDouble("spark.memory.storageFraction", 0.5)).toLong,
      numCores = numCores)
  }
```

我们继续深入看一下Storage + Execution 最大可使用内存是怎么计算的，进入 getMaxMemory(conf) 方法

```scala
  /**
   * Return the total amount of memory shared between execution and storage, in bytes.
   */
  private def getMaxMemory(conf: SparkConf): Long = {
    // TODO 获取 JVM 内存
    val systemMemory = conf.getLong("spark.testing.memory", Runtime.getRuntime.maxMemory)
    // TODO 获取预留内存，默认是 300M，可以通过 spark.testing.reservedMemory 参数配置
    val reservedMemory = conf.getLong("spark.testing.reservedMemory",
      if (conf.contains("spark.testing")) 0 else RESERVED_SYSTEM_MEMORY_BYTES)
    // TODO 最少需要的内存 = reservedMemory * 1.5 = 450M
    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong
    // TODO 如果 JVM 内存内存小于最小内存，则启动失败
    if (systemMemory < minSystemMemory) {
      throw new IllegalArgumentException(s"System memory $systemMemory must " +
        s"be at least $minSystemMemory. Please increase heap size using the --driver-memory " +
        s"option or spark.driver.memory in Spark configuration.")
    }
    // TODO 如果用户配置了 spark.executor.memory ,判断一下用户设置的值是否大于 最小内存，默认是 1G
    if (conf.contains("spark.executor.memory")) {
      val executorMemory = conf.getSizeAsBytes("spark.executor.memory")
      if (executorMemory < minSystemMemory) {
        throw new IllegalArgumentException(s"Executor memory $executorMemory must be at least " +
          s"$minSystemMemory. Please increase executor memory using the " +
          s"--executor-memory option or spark.executor.memory in Spark configuration.")
      }
    }
    // TODO 可用的内存 = JVM 内存 - 300M
    val usableMemory = systemMemory - reservedMemory
    // TODO 统一内存 占 可用内存的 60%
    val memoryFraction = conf.getDouble("spark.memory.fraction", 0.6)
    (usableMemory * memoryFraction).toLong
  }
```

通过阅读源码，可以发现跟我们上面的图一一对应了。

#### 4. TaskMemoryManager







#### 5. 内存管理篇之面试题

**为什么 Spark 比 MapReaduce 效率高？**

1. 基于内存
2. DAG

**引入堆外内存的好处？**

1. 提高内存利用率
2. 堆外内存不受JVM管理，减少FullGC，提高性能

**大数据领域两个比较常见优化方案？**

1. 引入堆外内存
2. 内存池化

**为什么 Spark 无法完全避免OOM？**

1. Spark 对堆内内存的管理是一种逻辑上的**"规划式"**的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存。所以 Spark 并不能准确的记录实际的内存使用。

2. 我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。



















