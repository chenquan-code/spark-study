## Spark源码



### 一、Spark RPC



#### 1. Akka RPC 实现

Akka 在 Scala 中的地位就像 Netty 在 Java 中的地位

Spark1.x 使用 Akka 实现底层的网络通信框架，Spark2.x 则改用 Netty 实现

Akka 是一个基于 scala 语言的异步的消息框架。

1. Akka 原理

   Akka 是基于事件驱动的异步消息模型

   ```
   1、它是对并发模型进行了更高的抽象；
   2、它是异步、非阻塞、高性能的事件驱动编程模型；
   3、它是轻量级事件处理(1GB 内存可容纳百万级别个 Actor);
   ```

   Akka 的组件

   ```
   ActorSystem: 管理和创建 Actor
   Actor：负责通信，内部有 MailBox 和 ActorRef 帮助完成通信，也可以创建 Actor
   
   ActorRef：通信代理
   MailBox：存放消息的地方，使得处理任务可以异步执行
   ```

   关于对 Akka 的 ActorSystem 和 Actor 的理解：

   ```
   1、ActorSystem 是管理 Actor 生命周期的组件，Actor 是负责进行通信的组件
   2、每个 Actor 都有一个 MailBox，别的 Actor 发送给它的消息都首先储存在 MailBox 中，通过这种方式可以实现异步通信。
   3、每个 Actor 是单线程的处理方式，不断的从 MailBox 拉取消息执行处理，所以对于 Actor的消息处理，不适合调用会阻塞的处理方法
   4、Actor 可以改变他自身的状态，可以接收消息，也可以发送消息，还可以生成新的 Actor
   5、每一个 ActorSystem 和 Actor 都在启动的时候会给定一个 name，如果要从 ActorSystem 中，获取一个 Actor，则通过以下的方式来进行 Actor 的
   获取：akka.tcp://actorsystem_name@bigdata02:9527/user/actor_name 来进行定位
   6、如果一个 Actor 要和另外一个 Actor 进行通信，则必须先获取对方 Actor 的 ActorRef 对象，然后通过该对象发送消息即可。
   7、通过 tell 发送异步消息，不接收响应，通过 ask 发送异步消息，得到 Future 返回，通过异步回到返回处理结果。
   8、其实每一个 ActorSystem 就是存在于 JVM 中，用来管理 Actor 通信组件生命周期
   9、Actor 通信单元：A 给 B 发送消息，同时 B 也可以给 A 返回消息，A 和 B 都是通信单元
   ```

   流程图

   ![image-20221206003303132](D:\WorkSpace\GitHubWorkSpace\spark-study\spark-nx\image\笔记\image-20221206003303132.png)

2. 使用 Akka 模拟实现 Spark Standalone 集群

   Spark Standalone 集群就是 Spark 自己管理资源的集群，是一个主从架构，Master 和 Worker

   

心跳机制：Worker 节点给 Master 节点发送心跳，Master 更新 Worker 最近心跳时间 

验活机制：Master 节点遍历自己维护的 Worker List ，找出最近心跳时间已经超过某个时间范围的节点做下线处理

#### 2. Spark RPC 实现

​		在 Spark 的内部的很多地方都涉及网络通信，比如 Spark 各个组件之间的互相通信，用户文件与 Jar 包的上传，节点之间的 Shuffle 过程，Block 数据的复制和备份。在 Spark 0.x 和 Spark 1.x 版本中，使用 Akka 轻松构建强有力的高并发和分布式网络通信，但是在 Spark 2.x 被移除了。Spark官方对此的描述是："Akka的依赖被移除了，因此用户可以使用任何版本的 Akka 来进行编程了。"

1. Spark 进程之间的通信

   ```
   1. driver和master，比如driver会想master发送RegisterApplication消息
   2. Worker和master，比如worker会向master上报worker上运行Executor信息
   3. executor和driver，executor运行在worker上，spark的tasks被分发到运行在各个Executor中，executor需要通过向driver发送任务运行结果。
   4. worker和worker的通信，task运行期间需要从其他地方fetch数据，这些数据是由运行在其他worker上的executor上的task产生，因此需要到worker上fetch数据
   ```

   



#### 3. Spark Standalone 集群启动源码

向Spark 集群提交一个 Application的时候，从jar包中指定的Main方法开始执行，遇到一个Action算子就会触发一个Job的提交，提交Job的时候会把 Job RDD根据算子的类型串起来，构建出一个有向无环图DAG，这个DAG就会交给Driver中的DAGScheduler做Stage切分变成多个Stage，然后遍历Stage，把每个Stage交给TaskScheduler，TaskScheduler将Stage转化成Task Set，发送到对应的Executor执行

1. 架构设计

   SparkContext有三个重要的成员变量，DAGScheduler、TaskScheduler、SchedulerBackend



Worker验活机制：当Worker被判定为下线之后，Master在下线处理前会将这个Worker上运行的Driver或者Executor进行容错处理，也就是发送给其他节点去启动一个新的，然后再下线这个Worker



### 二、Spark Application 提交和部署





### 四、Spark 内存模型

Spark 对堆内内存的管理是一种逻辑上的**"规划式"**的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存。

在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 SparkContext，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。

#### 1. 静态内存管理模型（StaticMemoryManager）

Spark 1.6 之前采用静态内存管理模型，其特点是：存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的。

![image-20221209231727834](D:\WorkSpace\GitHubWorkSpace\spark-study\spark-nx\image\笔记\image-20221209231727834.png)

堆外内存中可用的执行内存和存储内存占用的空间可以被精确计算，所以没有Other区域

1. **堆内内存中可用的存储内存**

   StorageMemory = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction = 43.2%

2. **堆内内存中可用的执行内存**

   ExecutionMemory = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction = 16%

3. **静态内存管理模型优缺点**

   缺点：虽然各个区域的内存可以调节，但是开发人员要非常熟悉Spark的内存分配机制，而且还要根据作业的规模做出合理的配置，如果配置不合理就会造成旱的旱死，涝的涝死。Spark 为了兼容老版本应用程序的目的仍然保留了这种内存模型的实现和使用。

#### 2. 统一内存管理模型（UnifiedMemoryManager）

Spark1.6之后引入统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域。

![image-20221209231657959](D:\WorkSpace\GitHubWorkSpace\spark-study\spark-nx\image\笔记\image-20221209231657959.png)

**动态占用机制**

1. 优先级：**Execution 内存 > Storage 内存**，意味着 执行内存 如果被  存储内存 占用了，可以要求归还，反之不能。
2. 存储内存 和 执行内存 在对方还有空闲而自己使用满的情况下都可以占用对方的内存，但是只有 执行内存 可以要求 存储内存 归还，
3. 如果 存储内存 和 执行内存 都不足，则他们会使用磁盘



#### 3. MemoryManager

UnifiedMemoryManager 内存模型内各个区域的划分，就是由MemoryManager 决定的，所以我们从 MemoryManager 入手

我们平时创建SparkSession的时候，内部会初始化 SparkContext，SparkContext 初始化的时候会创建 SparkEnv，我们的MemoryManager就是在SparkEnv里创建的，接下来我们就打开源码：org.apache.spark.SparkEnv#create

```scala
SparkEnv.create(){
	...
    // TODO useLegacyMemoryManager：是否使用过期的内存管理器，默认是否，所以创建的是UnifiedMemoryManager
    val useLegacyMemoryManager = conf.getBoolean("spark.memory.useLegacyMode", false)
    val memoryManager: MemoryManager =
    if (useLegacyMemoryManager) {
        new StaticMemoryManager(conf, numUsableCores)
    } else {
        UnifiedMemoryManager(conf, numUsableCores)
    }
}
```

我们继续跟进UnifiedMemoryManager(conf, numUsableCores)

```scala
  // TODO 系統保留内存 300M
  private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024

  def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = {
    // TODO 获取 (Storage + Execution) 最大可使用的内存 = ( heap size - 300M ) * 0.6
    val maxMemory = getMaxMemory(conf)
    new UnifiedMemoryManager(
      conf,
      // TODO 统一内存等于最大可使用内存
      maxHeapMemory = maxMemory,
      // TODO 存储内存占统一内存的 50%
      onHeapStorageRegionSize =
        (maxMemory * conf.getDouble("spark.memory.storageFraction", 0.5)).toLong,
      numCores = numCores)
  }
```

我们继续深入看一下Storage + Execution 最大可使用内存是怎么计算的，进入 getMaxMemory(conf) 方法

```scala
  /**
   * Return the total amount of memory shared between execution and storage, in bytes.
   */
  private def getMaxMemory(conf: SparkConf): Long = {
    // TODO 获取 JVM 内存
    val systemMemory = conf.getLong("spark.testing.memory", Runtime.getRuntime.maxMemory)
    // TODO 获取预留内存，默认是 300M，可以通过 spark.testing.reservedMemory 参数配置
    val reservedMemory = conf.getLong("spark.testing.reservedMemory",
      if (conf.contains("spark.testing")) 0 else RESERVED_SYSTEM_MEMORY_BYTES)
    // TODO 最少需要的内存 = reservedMemory * 1.5 = 450M
    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong
    // TODO 如果 JVM 内存内存小于最小内存，则启动失败
    if (systemMemory < minSystemMemory) {
      throw new IllegalArgumentException(s"System memory $systemMemory must " +
        s"be at least $minSystemMemory. Please increase heap size using the --driver-memory " +
        s"option or spark.driver.memory in Spark configuration.")
    }
    // TODO 如果用户配置了 spark.executor.memory ,判断一下用户设置的值是否大于 最小内存，默认是 1G
    if (conf.contains("spark.executor.memory")) {
      val executorMemory = conf.getSizeAsBytes("spark.executor.memory")
      if (executorMemory < minSystemMemory) {
        throw new IllegalArgumentException(s"Executor memory $executorMemory must be at least " +
          s"$minSystemMemory. Please increase executor memory using the " +
          s"--executor-memory option or spark.executor.memory in Spark configuration.")
      }
    }
    // TODO 可用的内存 = JVM 内存 - 300M
    val usableMemory = systemMemory - reservedMemory
    // TODO 统一内存 占 可用内存的 60%
    val memoryFraction = conf.getDouble("spark.memory.fraction", 0.6)
    (usableMemory * memoryFraction).toLong
  }
```

通过阅读源码，可以发现跟我们上面的图一一对应了。

#### 4. TaskMemoryManager







#### 5. 内存管理篇之面试题

**为什么 Spark 比 MapReaduce 效率高？**

1. 基于内存
2. DAG

**引入堆外内存的好处？**

1. 提高内存利用率
2. 堆外内存不受JVM管理，减少FullGC，提高性能

**大数据领域两个比较常见优化方案？**

1. 引入堆外内存
2. 内存池化

**为什么 Spark 无法完全避免OOM？**

1. Spark 对堆内内存的管理是一种逻辑上的**"规划式"**的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存。所以 Spark 并不能准确的记录实际的内存使用。

2. 我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。



















